{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9672991,"sourceType":"datasetVersion","datasetId":5911374},{"sourceId":229837824,"sourceType":"kernelVersion"},{"sourceId":230419618,"sourceType":"kernelVersion"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorchvideo\n\nfrom IPython import display\ndisplay.clear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:28:00.606656Z","iopub.execute_input":"2025-05-05T07:28:00.606914Z","iopub.status.idle":"2025-05-05T07:28:13.279184Z","shell.execute_reply.started":"2025-05-05T07:28:00.606892Z","shell.execute_reply":"2025-05-05T07:28:13.278393Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#-- Import Libraries -------------------------------------------------------------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.models.video import mvit_v2_s\nfrom torchvision.models import efficientnet_b2\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple\nimport os\nfrom sklearn.cluster import KMeans\nimport json \nfrom collections import defaultdict\n#---------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:31:55.666104Z","iopub.execute_input":"2025-05-05T07:31:55.666457Z","iopub.status.idle":"2025-05-05T07:32:04.635529Z","shell.execute_reply.started":"2025-05-05T07:31:55.666427Z","shell.execute_reply":"2025-05-05T07:32:04.634546Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#-- Initialize -----------------------------------------------------------------------------------------\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'device: {DEVICE}')\n\nNUM_CLASSES = 2\nCLASS_NAMES = ['normal', 'fight']\n\nNUM_FRAMES = 16\nFRAME_W = 224\nFRAME_H = 224\n\nTARGET_LAYER_INDEX = -1\nFRAME_INDICES = [1, 3, 5, 7, 9, 11, 13, 15]\nNUM_ATTENDED_FRAMES = 8\n#---------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:32:04.636983Z","iopub.execute_input":"2025-05-05T07:32:04.637473Z","iopub.status.idle":"2025-05-05T07:32:04.649355Z","shell.execute_reply.started":"2025-05-05T07:32:04.637445Z","shell.execute_reply":"2025-05-05T07:32:04.648408Z"}},"outputs":[{"name":"stdout","text":"device: cpu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#-- Mvit Model Definition ----------------------------------------------------------------------------------\nclass VideoMViTModel(nn.Module):\n    def __init__(self, num_classes=2):\n        super(VideoMViTModel, self).__init__()\n        self.model = mvit_v2_s(weights=\"DEFAULT\")\n        self.model.head = nn.Linear(768, num_classes)\n\n    def forward(self, x):\n        return self.model(x)\n#---------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:32:04.650009Z","iopub.execute_input":"2025-05-05T07:32:04.650283Z","iopub.status.idle":"2025-05-05T07:32:04.672262Z","shell.execute_reply.started":"2025-05-05T07:32:04.650259Z","shell.execute_reply":"2025-05-05T07:32:04.671293Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#-- Load MviT Model -----------------------------------------------------------------------------------------------\nweights_dir = \"/kaggle/input/fight-wholedata-fold4-60epoch/best_model_fold_4.pth\"\n\nmvit_model = VideoMViTModel(num_classes=2)  \ncheckpoint = torch.load(weights_dir, map_location=torch.device(DEVICE))\ncheckpoint = {k.replace('module.', ''): v for k, v in checkpoint.items()}\n\nmvit_model.load_state_dict(checkpoint, strict=False)\nmvit_model.eval()\n\nprint(':)')\n#---------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:32:04.673774Z","iopub.execute_input":"2025-05-05T07:32:04.674043Z","iopub.status.idle":"2025-05-05T07:32:08.249151Z","shell.execute_reply.started":"2025-05-05T07:32:04.674021Z","shell.execute_reply":"2025-05-05T07:32:08.248437Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/mvit_v2_s-ae3be167.pth\" to /root/.cache/torch/hub/checkpoints/mvit_v2_s-ae3be167.pth\n100%|██████████| 132M/132M [00:00<00:00, 174MB/s]  \n/tmp/ipykernel_31/1415934217.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(weights_dir, map_location=torch.device(DEVICE))\n","output_type":"stream"},{"name":"stdout","text":":)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#-- Load EfficentNet Model ----------------------------------------------------------------------------------\neff_model = efficientnet_b2(weights='IMAGENET1K_V1')  \neff_model = torch.nn.Sequential(*(list(eff_model.children())[:-2]), torch.nn.AdaptiveAvgPool2d((1, 1)))  #-- Remove top layers\neff_model = eff_model.to(DEVICE).eval()  \n#---------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:32:08.250448Z","iopub.execute_input":"2025-05-05T07:32:08.250875Z","iopub.status.idle":"2025-05-05T07:32:08.863248Z","shell.execute_reply.started":"2025-05-05T07:32:08.250854Z","shell.execute_reply":"2025-05-05T07:32:08.862543Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n100%|██████████| 35.2M/35.2M [00:00<00:00, 146MB/s] \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#-- Preprocessing for input frames -----------------------------------------------------------------------\ntransform = transforms.Compose([\n    transforms.ToPILImage(), \n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n])\n#---------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:32:11.397789Z","iopub.execute_input":"2025-05-05T07:32:11.398487Z","iopub.status.idle":"2025-05-05T07:32:11.404175Z","shell.execute_reply.started":"2025-05-05T07:32:11.398453Z","shell.execute_reply":"2025-05-05T07:32:11.403349Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#-- Function to  Extract all frames from a video ----------------------------------------------------------\ndef extract_frames(video_path):    \n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    success, frame = cap.read()\n    while success:\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  #-- Convert BGR to RGB\n        frames.append(transform(frame))\n        success, frame = cap.read()\n    cap.release()\n    return torch.stack(frames).to(DEVICE)\n#---------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:32:12.985576Z","iopub.execute_input":"2025-05-05T07:32:12.985900Z","iopub.status.idle":"2025-05-05T07:32:12.991110Z","shell.execute_reply.started":"2025-05-05T07:32:12.985868Z","shell.execute_reply":"2025-05-05T07:32:12.990079Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#-- Function to \"Extract features for a list of frames ------------------------------------------------------\ndef extract_features(frames, batch_size=32):    \n    features = []\n    num_frames = frames.shape[0]  #-- Total number of frames\n    for start in range(0, num_frames, batch_size):\n        # Process a batch of frames\n        end = min(start + batch_size, num_frames)\n        batch = frames[start:end].to(DEVICE)\n        with torch.no_grad():\n            batch_features = eff_model(batch).squeeze(-1).squeeze(-1)  #-- Remove extra dimensions\n        features.append(batch_features.cpu().numpy())  #-- Convert to NumPy and append\n    \n    return np.concatenate(features, axis=0)  #-- Combine features from all batches\n#---------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:32:14.862106Z","iopub.execute_input":"2025-05-05T07:32:14.862982Z","iopub.status.idle":"2025-05-05T07:32:14.870008Z","shell.execute_reply.started":"2025-05-05T07:32:14.862949Z","shell.execute_reply":"2025-05-05T07:32:14.868993Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#-- Function to Cluster the frames ---------------------------------------------------------------------------\ndef cluster_frames(features, n_clusters=16):    \n    num_frames = len(features)\n    print(\"number of frames\", num_frames)\n    if num_frames < 2:\n        return 1, [0]  #-- If there are fewer than 2 frames, return 1 cluster\n    \n    #-- Ensure the number of clusters is not greater than the number of frames --\n    n_clusters = min(n_clusters, num_frames)\n    \n    #-- Perform KMeans clustering --\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n    labels = kmeans.fit_predict(features)\n    \n    return n_clusters, labels\n#---------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:32:16.400215Z","iopub.execute_input":"2025-05-05T07:32:16.400802Z","iopub.status.idle":"2025-05-05T07:32:16.406054Z","shell.execute_reply.started":"2025-05-05T07:32:16.400776Z","shell.execute_reply":"2025-05-05T07:32:16.405127Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#-- function to get all frames in a cluster ---------------------------------------------------------------\ndef get_frames_per_cluster(labels):\n    cluster_dict = defaultdict(list)    \n    for frame_idx, label in enumerate(labels):\n        cluster_dict[label].append(frame_idx)\n        \n    return cluster_dict\n#---------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:32:56.194206Z","iopub.execute_input":"2025-05-05T07:32:56.194529Z","iopub.status.idle":"2025-05-05T07:32:56.198958Z","shell.execute_reply.started":"2025-05-05T07:32:56.194506Z","shell.execute_reply":"2025-05-05T07:32:56.198121Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#-- Function to Preprocess videos -------------------------------------------------------------------------------\ndef preprocess_video(video_path, num_frames=NUM_FRAMES, resize=(FRAME_W, FRAME_H)):    \n    \n    frames = extract_frames(video_path)\n    features = extract_features(frames)\n\n    n_clusters, labels = cluster_frames(features, n_clusters=num_frames)\n\n    cluster_dict = get_frames_per_cluster(labels)\n\n    representative_frames = []\n    frame_indices = []\n\n    for cluster in range(n_clusters):\n        cluster_indices = np.where(labels == cluster)[0]\n        if len(cluster_indices) > 0:\n            representative_index = cluster_indices[0]\n            representative_frames.append(frames[representative_index])\n            frame_indices.append(int(representative_index))  \n\n    # representative_frames_cpu = [frame.cpu().numpy() for frame in representative_frames]\n    # video_tensor = torch.stack(representative_frames)\n    # video_tensor = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)  #-- [1, 3, 16, 224, 224]\n\n    #- Now we sort the representative frames and their indices --\n    sorted_indices = np.argsort(frame_indices)  #-- Get sorted indices based on frame_indices\n    sorted_representative_frames = [representative_frames[i] for i in sorted_indices]\n    sorted_frame_indices = [frame_indices[i] for i in sorted_indices]\n\n    #-- Convert to tensor and adjust dimensions --\n    video_tensor = torch.stack(sorted_representative_frames)\n    video_tensor = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)  #-- [1, 3, 16, 224, 224]\n\n    return video_tensor, sorted_frame_indices, cluster_dict\n    # return video_tensor, frame_indices\n    \n#-------------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:34:32.145188Z","iopub.execute_input":"2025-05-05T07:34:32.146142Z","iopub.status.idle":"2025-05-05T07:34:32.153063Z","shell.execute_reply.started":"2025-05-05T07:34:32.146107Z","shell.execute_reply":"2025-05-05T07:34:32.152254Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#-- function to Predict video label ------------------------------------------------------------------------------\ndef predict_video(model, preprocessed_video, num_frames=NUM_FRAMES, resize=(FRAME_W, FRAME_H)):    \n    \n    with torch.no_grad():        \n        outputs = model(preprocessed_video)\n        _, predicted = torch.max(outputs, 1)      \n    \n    return CLASS_NAMES[predicted.item()]   \n#-----------------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:34:38.922396Z","iopub.execute_input":"2025-05-05T07:34:38.922699Z","iopub.status.idle":"2025-05-05T07:34:38.927269Z","shell.execute_reply.started":"2025-05-05T07:34:38.922677Z","shell.execute_reply":"2025-05-05T07:34:38.926513Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"#-- link of MviT Source Code --------------------------------------------------------------------------------------\n#https://github.com/pytorch/vision/blob/main/torchvision/models/video/mvit.py\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T07:06:08.084361Z","iopub.execute_input":"2025-04-27T07:06:08.08468Z","iopub.status.idle":"2025-04-27T07:06:08.088601Z","shell.execute_reply.started":"2025-04-27T07:06:08.084657Z","shell.execute_reply":"2025-04-27T07:06:08.087701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Required functions for forward from Source Code --------------------------------------------------------------\n#------------------------------------------------------------------------\ndef _add_rel_pos(\n    attn: torch.Tensor,\n    q: torch.Tensor,\n    q_thw: Tuple[int, int, int],\n    k_thw: Tuple[int, int, int],\n    rel_pos_h: torch.Tensor,\n    rel_pos_w: torch.Tensor,\n    rel_pos_t: torch.Tensor,) -> torch.Tensor:\n    # Modified code from: https://github.com/facebookresearch/SlowFast/commit/1aebd71a2efad823d52b827a3deaf15a56cf4932\n    q_t, q_h, q_w = q_thw\n    k_t, k_h, k_w = k_thw\n    dh = int(2 * max(q_h, k_h) - 1)\n    dw = int(2 * max(q_w, k_w) - 1)\n    dt = int(2 * max(q_t, k_t) - 1)\n\n    # Scale up rel pos if shapes for q and k are different.\n    q_h_ratio = max(k_h / q_h, 1.0)\n    k_h_ratio = max(q_h / k_h, 1.0)\n    dist_h = torch.arange(q_h)[:, None] * q_h_ratio - (torch.arange(k_h)[None, :] + (1.0 - k_h)) * k_h_ratio\n    q_w_ratio = max(k_w / q_w, 1.0)\n    k_w_ratio = max(q_w / k_w, 1.0)\n    dist_w = torch.arange(q_w)[:, None] * q_w_ratio - (torch.arange(k_w)[None, :] + (1.0 - k_w)) * k_w_ratio\n    q_t_ratio = max(k_t / q_t, 1.0)\n    k_t_ratio = max(q_t / k_t, 1.0)\n    dist_t = torch.arange(q_t)[:, None] * q_t_ratio - (torch.arange(k_t)[None, :] + (1.0 - k_t)) * k_t_ratio\n\n    # Interpolate rel pos if needed.\n    rel_pos_h = _interpolate(rel_pos_h, dh)\n    rel_pos_w = _interpolate(rel_pos_w, dw)\n    rel_pos_t = _interpolate(rel_pos_t, dt)\n    Rh = rel_pos_h[dist_h.long()]\n    Rw = rel_pos_w[dist_w.long()]\n    Rt = rel_pos_t[dist_t.long()]\n\n    B, n_head, _, dim = q.shape\n\n    r_q = q[:, :, 1:].reshape(B, n_head, q_t, q_h, q_w, dim)\n    rel_h_q = torch.einsum(\"bythwc,hkc->bythwk\", r_q, Rh)  # [B, H, q_t, qh, qw, k_h]\n    rel_w_q = torch.einsum(\"bythwc,wkc->bythwk\", r_q, Rw)  # [B, H, q_t, qh, qw, k_w]\n    # [B, H, q_t, q_h, q_w, dim] -> [q_t, B, H, q_h, q_w, dim] -> [q_t, B*H*q_h*q_w, dim]\n    r_q = r_q.permute(2, 0, 1, 3, 4, 5).reshape(q_t, B * n_head * q_h * q_w, dim)\n    # [q_t, B*H*q_h*q_w, dim] * [q_t, dim, k_t] = [q_t, B*H*q_h*q_w, k_t] -> [B*H*q_h*q_w, q_t, k_t]\n    rel_q_t = torch.matmul(r_q, Rt.transpose(1, 2)).transpose(0, 1)\n    # [B*H*q_h*q_w, q_t, k_t] -> [B, H, q_t, q_h, q_w, k_t]\n    rel_q_t = rel_q_t.view(B, n_head, q_h, q_w, q_t, k_t).permute(0, 1, 4, 2, 3, 5)\n\n    # Combine rel pos.\n    rel_pos = (\n        rel_h_q[:, :, :, :, :, None, :, None]\n        + rel_w_q[:, :, :, :, :, None, None, :]\n        + rel_q_t[:, :, :, :, :, :, None, None]\n    ).reshape(B, n_head, q_t * q_h * q_w, k_t * k_h * k_w)\n\n    # Add it to attention\n    attn[:, :, 1:, 1:] += rel_pos\n\n    return attn\n#------------------------------------------------------------------------\n\n#------------------------------------------------------------------------\ndef _interpolate(embedding: torch.Tensor, d: int) -> torch.Tensor:\n    if embedding.shape[0] == d:\n        return embedding\n\n    return (\n        nn.functional.interpolate(\n            embedding.permute(1, 0).unsqueeze(0),\n            size=d,\n            mode=\"linear\",\n        )\n        .squeeze(0)\n        .permute(1, 0)\n    )\n#------------------------------------------------------------------------\n\n#------------------------------------------------------------------------\ndef _add_shortcut(x: torch.Tensor, shortcut: torch.Tensor, residual_with_cls_embed: bool):\n    if residual_with_cls_embed:\n        x.add_(shortcut)\n    else:\n        x[:, :, 1:, :] += shortcut[:, :, 1:, :]\n    return x\n#------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:34:40.840304Z","iopub.execute_input":"2025-05-05T07:34:40.840899Z","iopub.status.idle":"2025-05-05T07:34:40.854033Z","shell.execute_reply.started":"2025-05-05T07:34:40.840873Z","shell.execute_reply":"2025-05-05T07:34:40.853281Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#--Override Forward Function to get Attention Map and Cls weights -----------------------------------------------------\ndef my_forward_wrapper_video(attn_obj):\n    \n    def my_forward(x, thw):\n        B, N, C = x.shape\n\n        q, k, v = attn_obj.qkv(x).reshape(B, N, 3, attn_obj.num_heads, attn_obj.head_dim).transpose(1, 3).unbind(dim=2)\n        if attn_obj.pool_k is not None:\n            k, k_thw = attn_obj.pool_k(k, thw)\n        else:\n            k_thw = thw\n        if attn_obj.pool_v is not None:\n            v = attn_obj.pool_v(v, thw)[0]\n        if attn_obj.pool_q is not None:\n            q, thw = attn_obj.pool_q(q, thw)        \n        \n\n        attn = torch.matmul(attn_obj.scaler * q, k.transpose(2, 3))\n        if attn_obj.rel_pos_h is not None and attn_obj.rel_pos_w is not None and attn_obj.rel_pos_t is not None:\n            attn = _add_rel_pos(\n                attn,\n                q,\n                thw,\n                k_thw,\n                attn_obj.rel_pos_h,\n                attn_obj.rel_pos_w,\n                attn_obj.rel_pos_t,\n            )\n        attn = attn.softmax(dim=-1)\n\n        \n        \n        attn_obj.attn_map = attn\n        attn_obj.cls_attn_map = attn[:, :, 0, 1:]\n        \n        # print('attn:', attn.shape)        \n        # print('attn_obj.cls_attn_map:', attn_obj.cls_attn_map.shape)        \n\n        x = torch.matmul(attn, v)\n        if attn_obj.residual_pool:\n            _add_shortcut(x, q, attn_obj.residual_with_cls_embed)\n        x = x.transpose(1, 2).reshape(B, -1, attn_obj.output_dim)\n        x = attn_obj.project(x)\n\n        return x, thw\n    \n    return my_forward\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:34:43.072258Z","iopub.execute_input":"2025-05-05T07:34:43.072571Z","iopub.status.idle":"2025-05-05T07:34:43.079969Z","shell.execute_reply.started":"2025-05-05T07:34:43.072550Z","shell.execute_reply":"2025-05-05T07:34:43.079137Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#-- Function to Plot HeatMaps on Frames ------------------------------------------------------------------------\ndef plot_attention_maps(img, attns, frame_idx):    \n    num_heads = attns.shape[0]\n    num_cols = 4  # تعداد ستون‌ها\n    num_rows = math.ceil(num_heads / num_cols)  \n    \n    plt.figure(figsize=(15, num_rows * 4))  \n    for head in range(num_heads):\n        row = head // num_cols  \n        col = head % num_cols   \n        \n        plt.subplot(num_rows, num_cols, head + 1)  \n        plt.imshow(img)\n        plt.imshow(attns[head].cpu().numpy().squeeze(0), cmap='jet', alpha=0.6)  \n        plt.title(f\"Head {head+1} - Frame {frame_idx}\")\n        plt.axis('off')\n    \n    plt.show()\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:34:48.374082Z","iopub.execute_input":"2025-05-05T07:34:48.374410Z","iopub.status.idle":"2025-05-05T07:34:48.380271Z","shell.execute_reply.started":"2025-05-05T07:34:48.374384Z","shell.execute_reply":"2025-05-05T07:34:48.379548Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"#-- function to Unnormalize frames -----------------------------------------------------------------\ndef unnormalize(tensor):\n    device = tensor.device  # گرفتن دیوایس تنسور ورودی\n    mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225], device=device).view(3, 1, 1)\n    return tensor * std + mean\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:34:57.898021Z","iopub.execute_input":"2025-05-05T07:34:57.898334Z","iopub.status.idle":"2025-05-05T07:34:57.903185Z","shell.execute_reply.started":"2025-05-05T07:34:57.898292Z","shell.execute_reply":"2025-05-05T07:34:57.902513Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"#-- Apply Custom Forward on Target Layer ----------------------------------------------------------------------------\nmvit_model.model.blocks[TARGET_LAYER_INDEX].attn.forward = my_forward_wrapper_video(mvit_model.model.blocks[TARGET_LAYER_INDEX].attn)\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:34:59.961530Z","iopub.execute_input":"2025-05-05T07:34:59.961824Z","iopub.status.idle":"2025-05-05T07:34:59.965914Z","shell.execute_reply.started":"2025-05-05T07:34:59.961804Z","shell.execute_reply":"2025-05-05T07:34:59.965207Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"#-- Function to get all cluster mates for a frame index ----------------------------------------------------\ndef get_cluster_mates_from_dict(frame_idx, cluster_dict):\n    for cluster_label, frame_list in cluster_dict.items():\n        if frame_idx in frame_list:\n            return frame_list\n    return []  \n#-----------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:36:00.284599Z","iopub.execute_input":"2025-05-05T07:36:00.284912Z","iopub.status.idle":"2025-05-05T07:36:00.289549Z","shell.execute_reply.started":"2025-05-05T07:36:00.284889Z","shell.execute_reply":"2025-05-05T07:36:00.288734Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"#-----------------------------------------------------------------------------------------------------------\ndef visualize_heatmaps_on_frames(cls_weight, all_frames, frame_indices):\n    patches_per_frame = cls_weight.shape[2] // NUM_ATTENDED_FRAMES  \n\n    framewise_attn = []\n    for i in range(NUM_ATTENDED_FRAMES):\n        start = i * patches_per_frame\n        end = (i + 1) * patches_per_frame\n        frame_attn = cls_weight[0, :, start:end]  \n        framewise_attn.append(frame_attn)\n    \n    combined_imgs = {}\n    for i, frame_attn in enumerate(framewise_attn):\n        for j in range(2):  \n            idx = i * 2 + j  \n            frame_idx = frame_indices[idx]\n    \n            print(f\"Processing frame {frame_idx}\")\n\n            mates = get_cluster_mates_from_dict(frame_idx, cluster_dict)\n\n            for f_idx in mates:            \n                x = all_frames[f_idx]\n                \n                cls_resized_all_heads = []\n                for head in range(frame_attn.shape[0]):\n                    cls_weight_head = frame_attn[head]\n                    cls_weight_2d = cls_weight_head.view(1, 7, 7).unsqueeze(0)\n                    cls_resized = F.interpolate(cls_weight_2d, size=(224, 224), mode='bilinear', align_corners=False)\n                    cls_resized_all_heads.append(cls_resized)\n        \n                cls_resized_all_heads = torch.stack(cls_resized_all_heads)\n                cls_resized_normalized = cls_resized_all_heads / cls_resized_all_heads.max()\n        \n                mean_attn_map = cls_resized_normalized.mean(dim=0)                  \n                x = unnormalize(x)\n                img_resized = x.permute(1, 2, 0).cpu().numpy()\n                img_resized = np.clip(img_resized, 0, 1)\n                img_resized = (img_resized * 255).astype(np.uint8)        \n        \n                mean_attn_map_resized = mean_attn_map.squeeze().cpu().numpy()\n                mean_attn_map_resized = (mean_attn_map_resized * 255 / mean_attn_map_resized.max()).astype(np.uint8)\n                        \n                attn_map_colored = cv2.applyColorMap(mean_attn_map_resized, cv2.COLORMAP_JET)       \n                combined_image = cv2.addWeighted(img_resized, 0.6, attn_map_colored, 0.4, 0)   \n        \n                combined_imgs[f_idx]= combined_image\n\n    return combined_imgs\n #-----------------------------------------------------------------------------------------------------------   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:39:24.933783Z","iopub.execute_input":"2025-05-05T07:39:24.934702Z","iopub.status.idle":"2025-05-05T07:39:24.943261Z","shell.execute_reply.started":"2025-05-05T07:39:24.934674Z","shell.execute_reply":"2025-05-05T07:39:24.942392Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"#-----------------------------------------------------------------------------------------------------------\ndef save_video_from_frames_dict(frames_dict, output_path, fps):\n    \n    sorted_items = sorted(frames_dict.items())\n    \n    first_frame = sorted_items[0][1]\n    height, width, _ = first_frame.shape\n    \n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  \n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    for _, frame in sorted_items:\n        if frame.dtype != np.uint8:\n            frame = (frame * 255).astype(np.uint8)\n        out.write(frame)\n\n    out.release()\n    print(f\"Video saved at: {output_path}\")\n#-----------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:39:40.037958Z","iopub.execute_input":"2025-05-05T07:39:40.038299Z","iopub.status.idle":"2025-05-05T07:39:40.044080Z","shell.execute_reply.started":"2025-05-05T07:39:40.038274Z","shell.execute_reply":"2025-05-05T07:39:40.043178Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"#-----------------------------------------------------------------------------------------------------------\ndef get_video_fps(video_path):\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        raise IOError(f\"Cannot open video: {video_path}\")\n    \n    fps = cap.get(cv2.CAP_PROP_FPS)\n    cap.release()\n    return fps\n#-----------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:39:48.534029Z","iopub.execute_input":"2025-05-05T07:39:48.534375Z","iopub.status.idle":"2025-05-05T07:39:48.539079Z","shell.execute_reply.started":"2025-05-05T07:39:48.534351Z","shell.execute_reply":"2025-05-05T07:39:48.538473Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"#-- Run ---------------------------------------------------------------------------------------------------------\n# video_path = '/kaggle/input/sample-videos-for-fight-detection-2/fight/fight (2).mp4'\nvideo_path = '/kaggle/input/sample-videos-for-fight-detection-2/normal/normal (1).mp4'\noutput_video_path = 'output_with_attention_map.mp4'\n\nframe_width = 224  # Assuming the frames are 224x224\nframe_height = 224\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_video_path, fourcc, 20.0, (frame_width, frame_height))\n\nall_frames = extract_frames(video_path)\nprint('all_frames:', len(all_frames), all_frames[0].shape)\nvideo_tensor, frame_indices, cluster_dict = preprocess_video(video_path)\nprint('video_tensor:', video_tensor.shape)\nprint('frame_indices:', frame_indices)\n\npredicted_lbl = predict_video(mvit_model , video_tensor)\nprint('predicted_lbl:', predicted_lbl)\n\n#-- Get the attention map and cls_weight --\nattn_map = mvit_model.model.blocks[-1].attn.attn_map.mean(dim=1).squeeze(0).detach()\ncls_weight = mvit_model.model.blocks[-1].attn.cls_attn_map  \n\nprint('attn_map:' , attn_map.shape)\nprint('cls_weight:', cls_weight.shape)\n\n\n\ncombined_imgs = visualize_heatmaps_on_frames(cls_weight, all_frames, frame_indices)\ncombined_imgs = dict(sorted(combined_imgs.items()))\n# print(combined_imgs)\n# for frame_idx , img in combined_imgs.items():\n#         # رسم تصویر ترکیب‌شده\n#         plt.figure(figsize=(5, 5))\n#         # plt.imshow(combined_image)\n#         plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n#         plt.title(f\"Mean Attention - Frame {frame_idx}\")\n#         plt.axis('off')\n#         plt.show()\n\nfps = get_video_fps(video_path)\nsave_video_from_frames_dict(combined_imgs, output_video_path, fps)\n#-----------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T07:41:27.897721Z","iopub.execute_input":"2025-05-05T07:41:27.898033Z","iopub.status.idle":"2025-05-05T07:41:47.127918Z","shell.execute_reply.started":"2025-05-05T07:41:27.898009Z","shell.execute_reply":"2025-05-05T07:41:47.127089Z"}},"outputs":[{"name":"stdout","text":"all_frames: 158 torch.Size([3, 224, 224])\nnumber of frames 158\nvideo_tensor: torch.Size([1, 3, 16, 224, 224])\nframe_indices: [0, 17, 26, 37, 45, 57, 68, 78, 87, 96, 118, 119, 126, 135, 141, 153]\npredicted_lbl: normal\nattn_map: torch.Size([393, 393])\ncls_weight: torch.Size([1, 8, 392])\nProcessing frame 0\nProcessing frame 17\nProcessing frame 26\nProcessing frame 37\nProcessing frame 45\nProcessing frame 57\nProcessing frame 68\nProcessing frame 78\nProcessing frame 87\nProcessing frame 96\nProcessing frame 118\nProcessing frame 119\nProcessing frame 126\nProcessing frame 135\nProcessing frame 141\nProcessing frame 153\nVideo saved at: output_with_attention_map.mp4\n","output_type":"stream"}],"execution_count":28}]}